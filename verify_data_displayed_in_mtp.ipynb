{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation 3:  Verify that data appears as expected within the MTP UI\n",
    "Started 2022-10-28 ZD  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "### \"Does the MTP UI match the MTP database?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "This validation will test “completeness and accuracy of data loaded into the platform” and follow MTP Data Validations 1&2. It will compare the data displayed within the platform GUI (after loading) to the expected values within the data (before loading). Automated scripts will pull test cases from the data that can be fed into platform testing automations to check displays for completeness and accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "DV3 will focus on displays within the platform that relate to new pediatric data, including those that happen to incorporate Open Targets (OT) data. New pediatric data includes the Food and Drug Administration’s Pediatric Molecular Target Lists (FDA PMTL); the much larger collection of evidence data provided by the Children’s Hospital of Philadelphia (CHoP); and derived summary tables, such as the Pediatric Cancer Data Navigation (PCDN) page. It will not validate or test displays that only include OT data without pediatric data.  \n",
    "\n",
    "The testing within DV3 will use sampling (spot-checking) methods, though scalability to meet automation capacity will be a design goal. Samples tested will include a set of defined high-profile genes and diseases that we expect will garner an abundance of user attention. We will also include a random sampling of genes and diseases (associated with CHoP data) to expand testing scope."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case Overview\n",
    "\n",
    "Testable values for each test case will be contained in a tab within the output Excel\n",
    "\n",
    "1. Target Association\n",
    "    - Count of associated diseases\n",
    "    - PMTL annotation\n",
    "2. Target Profile Page\n",
    "    - Somatic Alterations widget\n",
    "        - Status\n",
    "        - Row count of each of 5 tabs\n",
    "    - Gene Expression widget\n",
    "        - Status\n",
    "    - Epigenetic Modification widget\n",
    "        - Status\n",
    "        - Row count of each of 2 tabs\n",
    "    - Differential Expression widget\n",
    "        -Status\n",
    "3. Disease Association\n",
    "    - Count of associated targets\n",
    "4. Disease Profile Page\n",
    "    - Differential Expression widget\n",
    "        -Status\n",
    "5. Evidence Page\n",
    "    - Somatic Alterations widget\n",
    "        - Status\n",
    "        - Row count of each of 5 tabs\n",
    "    - Gene Expression widget\n",
    "        - Status\n",
    "    - Epigenetic Modification widget\n",
    "        - Status\n",
    "        - Row count of each of 2 tabs\n",
    "6. Pediatric Cancer Data Navigation (PCDN) Page\n",
    "    - Row count of resulting evidence pages when searching for target or disease\n",
    "7. Pediatric Molecular Targets List (PMTL) Page\n",
    "    - Row count of Relevant and Non-Relevant targets  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates\n",
    "\n",
    "**2022-11-15 (initial changes)**\n",
    "- Generate and export new PCDN instead of ingesting\n",
    "- Add random seed for reproducible random results\n",
    "- Change Excel export to function with dictionary input for more flexibility\n",
    "\n",
    "**2022-11-16**\n",
    "- Update PCDN export functions\n",
    "    - Export PCDN as partitioned JSONL files\n",
    "    - Compress files into one folder\n",
    "    - Create md5 checksum\n",
    "- Edit output data file structure\n",
    "\n",
    "**2022-11-21**\n",
    "- Add 'id' field to PCDN with uuids for each row\n",
    "\n",
    "**2022-11-22**\n",
    "- Reorganize data folders to allow for separate versioning of CHoP data types\n",
    "- Update output filenames to identify data type versions instead of only OpenPedCan version\n",
    "- Add function to build and export PCDN disease list for drop-down menu\n",
    "- Add function to build and export PCDN target, disease, and evidence counts\n",
    "\n",
    "**2022-12-05**\n",
    "- Rebuild Target page Associations test case to use OT AOTF as source file instead of \n",
    "precalculated OT Direct Overall Associations\n",
    "- Known issues remain:\n",
    "    - Disease page Associations test case should be rebuilt to use AOTf file and impose\n",
    "    indirect association (ontology) logic (DONE 1/25/2023)\n",
    "    - Evidence page test case should be rebuild to include indirect association logic (DONE 1/26/2023)\n",
    "    - PCDN Gene search test case should be rebuilt to match site logic of prefix search (DONE 1/27/2023)\n",
    "\n",
    "**2023-01-25**\n",
    "- Change ot_diseases filter to keep 'descendants' column. Propagates to all downstream joins\n",
    "- Add function to search for indirect evidence using disease ontology descendants\n",
    "- Rebuild Disease page Associations test to create indirect associations using AOTF evidence\n",
    "\n",
    "**2023-01-26**\n",
    "- Fix typos and adjust list spacing and indentation for readability\n",
    "- Rebuild Evidence test cases to gather indirect evidence\n",
    "\n",
    "**2023-01-27**\n",
    "- Rebuild PCDN target test to match MTP PCDN search behavior, which includes similar \n",
    "hypnated targets in search results\n",
    "- Add tests to check that targets, diseases, and evidence defined in dv3_priority_tests are both valid and useful\n",
    "- Increased standard number of test cases randomly samples (total cases from 20-40 for most tests)\n",
    "\n",
    "**2023-02-17**\n",
    "- Add option to diseaseAssc (Disease page Associations) function use all pediatric diseases present in \n",
    "CHoP (non-OT) data instead of few priority diseases listed in csv\n",
    "\n",
    "**2023-02-22**\n",
    "- Add Epigenetic Modification (Methylation) data to test cases and PCDN\n",
    "    - Include Methyl by Gene and Methyl by Isoform into loading/cleaning/combining steps\n",
    "    - Add targetProfile columns to output xlsx for sums and widget presence\n",
    "    - Add evidence columns to output xlsx for sums and widget presence\n",
    "    - Add EM version tags to output filenames\n",
    "\n",
    "**2023-03-13**\n",
    "- Add function and paths to export versioned csvs of OpenTargets disease and target lists for ad-hoc analyses\n",
    "\n",
    "**2023-03-29**\n",
    "- Change name format of methylation files received from CHoP\n",
    "\n",
    "**2023-04-03**\n",
    "- Add function to load jsonl as chunks, aggregate, and then save as smaller grouped files to manage the large methylation files\n",
    "- Improve chop cleaning functions\n",
    "    - Fix datasourceId calling to prevent bug if first row is dropped\n",
    "    - Add option to accurately report evidence counts in aggregated methylation data\n",
    "    - Add gene symbols and disease names of blank IDs to text output\n",
    "- Add functionality to build_test_case_df to sum evidenceCounts of methylation data instead of counting rows\n",
    "- Add tqdm import and code to enable displaying progress bars to improve sanity\n",
    "- Change loading of GeneExpression files to use chunkloading for better performance\n",
    "\n",
    "**2023-05-04**\n",
    "- Add Differential Expression (DE) to path configs and data loading\n",
    "\n",
    "**2023-05-08**\n",
    "- Add optional parameter to group by custom fields in jsonl loading steps\n",
    "- Add DE to data cleaning steps\n",
    "- Fix randomSeed calling within test case functions\n",
    "\n",
    "**2023-05-10**\n",
    "- Add DE to PCDN steps\n",
    "- Update priority disease and evidence csvs to reflect v12.0 changes\n",
    "\n",
    "**2023-05-11**\n",
    "- Add DE to targetProfile test case generation\n",
    "- Create new diseaseProfile test case and include DE data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and define relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import hashlib\n",
    "import shutil\n",
    "import uuid\n",
    "\n",
    "import ndjson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE VERSIONS\n",
    "OT_VERSION = '22.11'\n",
    "OPENPEDCAN_SOMATIC_ALTERATIONS_VERSION = 'v12.0'\n",
    "OPENPEDCAN_GENE_EXPRESSION_VERSION = 'v12.0'\n",
    "OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION = 'v12.0'\n",
    "OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION = 'v12.0'\n",
    "PMTL_VERSION = 'v3.1'\n",
    "RANDOM_SEED = 5555\n",
    "\n",
    "# --------\n",
    "\n",
    "# INPUTS\n",
    "\n",
    "# Data from Open Targets\n",
    "OT_PATH = 'data/external/opentargets/platform/' + OT_VERSION + '/output/etl/json/'\n",
    "\n",
    "OT_DISEASES_PATH = OT_PATH + 'diseases/'\n",
    "OT_TARGETS_PATH = OT_PATH + 'targets/'\n",
    "OT_AOTF_PATH = OT_PATH + 'AOTFClickhouse/'\n",
    "\n",
    "\n",
    "# Data from CHoP\n",
    "CHOP_SA_PATH = 'data/raw/chopOpenPedCan/somaticAlterations/' + OPENPEDCAN_SOMATIC_ALTERATIONS_VERSION + '/'\n",
    "CHOP_GX_PATH = 'data/raw/chopOpenPedCan/geneExpression/' + OPENPEDCAN_GENE_EXPRESSION_VERSION + '/'\n",
    "CHOP_EM_PATH = 'data/raw/chopOpenPedCan/epigeneticModification/' + OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION + '/'\n",
    "CHOP_DE_PATH = 'data/raw/chopOpenPedCan/differentialExpression/' + OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION + '/'\n",
    "\n",
    "\n",
    "# CHoP: Somatic Alterations\n",
    "CNV_PATH = CHOP_SA_PATH + 'gene-level-cnv-consensus-annotated-mut-freq.jsonl.gz'\n",
    "SNVGENE_PATH = CHOP_SA_PATH + 'gene-level-snv-consensus-annotated-mut-freq.jsonl.gz'\n",
    "SNV_PATH = CHOP_SA_PATH + 'variant-level-snv-consensus-annotated-mut-freq.jsonl.gz'\n",
    "FUSIONGENE_PATH = CHOP_SA_PATH + 'putative-oncogene-fused-gene-freq.jsonl.gz'\n",
    "FUSION_PATH = CHOP_SA_PATH + 'putative-oncogene-fusion-freq.jsonl.gz'\n",
    "\n",
    "\n",
    "# CHoP: Gene Expression\n",
    "TPMGENE_PATH = CHOP_GX_PATH + 'long_n_tpm_mean_sd_quantile_gene_wise_zscore.jsonl.gz'\n",
    "TPMGROUP_PATH = CHOP_GX_PATH + 'long_n_tpm_mean_sd_quantile_group_wise_zscore.jsonl.gz'\n",
    "\n",
    "\n",
    "# CHoP: Epigenetic Modification (Methylation)\n",
    "METHYL_FILENAME = 'isoform-methyl-beta-values-summary'\n",
    "METHYLGENE_FILENAME = 'gene-methyl-beta-values-summary'\n",
    "\n",
    "# CHoP: Raw Methylation (large files)\n",
    "METHYL_PATH = CHOP_EM_PATH + METHYL_FILENAME + '.jsonl.gz'\n",
    "METHYLGENE_PATH = CHOP_EM_PATH + METHYLGENE_FILENAME + '.jsonl.gz'\n",
    "\n",
    "# CHoP: Grouped Methylation (aggregated small files)\n",
    "GROUPED_CHOP_EM_PATH = 'data/processed/chopOpenPedCan/epigeneticModification_grouped/' + OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION + '/'\n",
    "GROUPED_METHYL_PATH = GROUPED_CHOP_EM_PATH + METHYL_FILENAME + '/'\n",
    "GROUPED_METHYLGENE_PATH = GROUPED_CHOP_EM_PATH + METHYLGENE_FILENAME + '/'\n",
    "\n",
    "\n",
    "# CHoP: Differential Expression\n",
    "DIFFEXPR_FILENAME = 'gene-counts-rsem-expected_count-collapsed-deseq'\n",
    "\n",
    "# CHoP: Raw Differential Expression (large files)\n",
    "DIFFEXPR_PATH = CHOP_DE_PATH + DIFFEXPR_FILENAME + '.jsonl.gz'\n",
    "\n",
    "# CHoP: Grouped Differential Expression (aggregated small files)\n",
    "GROUPED_CHOP_DE_PATH = 'data/processed/chopOpenPedCan/differentialExpression_grouped/' + OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION + '/'\n",
    "GROUPED_DIFFEXPR_PATH = GROUPED_CHOP_DE_PATH + DIFFEXPR_FILENAME + '/'\n",
    "\n",
    "\n",
    "\n",
    "# PMTL data\n",
    "PMTL_PATH = 'data/processed/pmtl/pmtl_' + PMTL_VERSION + '.json'\n",
    "\n",
    "# Priority targets and diseases for test cases\n",
    "PRIORITY_PATH = 'data/processed/dv3_priority_tests/'\n",
    "PRIORITY_TARGETS_PATH = PRIORITY_PATH + 'targets.csv'\n",
    "PRIORITY_DISEASES_PATH = PRIORITY_PATH + 'diseases.csv'\n",
    "PRIORITY_EVIDENCES_PATH = PRIORITY_PATH + 'evidences.csv'\n",
    "\n",
    "# --------\n",
    "\n",
    "# OUTPUTS\n",
    "\n",
    "# Versioned Open Targets Target/Disease csvs\n",
    "OT_TABLES_EXPORT_PATH = 'data/processed/ot_lists/' + OT_VERSION + '/'\n",
    "OT_TARGET_EXPORT_PATH = OT_TABLES_EXPORT_PATH + 'targets.csv'\n",
    "OT_DISEASE_EXPORT_PATH = OT_TABLES_EXPORT_PATH + 'diseases.csv'\n",
    "\n",
    "# Excel file for DV3 automation input\n",
    "XLSX_OUTPUT =   ('dv3_test_cases/MTP_DV3_' + OT_VERSION + \n",
    "                '_SA' + OPENPEDCAN_SOMATIC_ALTERATIONS_VERSION + \n",
    "                '_GX' + OPENPEDCAN_GENE_EXPRESSION_VERSION + \n",
    "                '_EM' + OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION +\n",
    "                '_DE' + OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION +\n",
    "                '.xlsx')\n",
    "\n",
    "# Pediatric Cancer Data Navigation\n",
    "PCDN_PATH =     ('data/processed/pcdn/' + OT_VERSION + \n",
    "                '_SA' + OPENPEDCAN_SOMATIC_ALTERATIONS_VERSION + \n",
    "                '_GX' + OPENPEDCAN_GENE_EXPRESSION_VERSION + \n",
    "                '_EM' + OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION +\n",
    "                '_DE' + OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION +\n",
    "                '/')\n",
    "PCDN_JSON = PCDN_PATH + 'chopDataNavigationTable.json'\n",
    "PCDN_COUNTS = PCDN_PATH + 'pcdnCounts.json'\n",
    "PCDN_DISEASES = PCDN_PATH + 'diseaseOptions.json'\n",
    "PCDN_JSONL = PCDN_PATH + 'jsonl_files'\n",
    "PCDN_JSONL_TEMP = PCDN_JSONL + '/'\n",
    "PCDN_JSONL_FILENAMES = 'pcdn_part_{0:04d}.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_files_to_df(path:str, \n",
    "                           filetype:str='*.json', \n",
    "                           progress:bool=False, \n",
    "                           aggData:bool=False, \n",
    "                           group_cols:list=['targetFromSourceId', \n",
    "                                            'Gene_symbol', \n",
    "                                            'diseaseFromSourceMappedId', \n",
    "                                            'Disease', \n",
    "                                            'datasourceId']):\n",
    "    \"\"\"\n",
    "    Load multiple identically-structured jsonl files within a local folder \n",
    "    into a single dataframe. Useful for OpenTargets FTP downloads.\n",
    "\n",
    "    :param path: Relative filepath to the folder containing the jsonl files.\n",
    "    :param filetype: Filetype suffix of files to include. default '*.json'\n",
    "    :param progress: If True, outputs tqdm progress bar while loading\n",
    "    :param aggData: If True, perform additonal groupby function. Use when \n",
    "        loading data already aggregated into chunks. \n",
    "    :param group_cols: list of columns by which to group agg data\n",
    "    \"\"\"\n",
    "    \n",
    "    # OT uses 'json' extension for 'jsonl' files\n",
    "    fullPath = path + filetype\n",
    "\n",
    "    # Create list of all files within path folder\n",
    "    files = glob.glob(fullPath)\n",
    "\n",
    "    # Build df by combining all files in path folder\n",
    "    if progress == False:\n",
    "        df = pd.concat(\n",
    "            (pd.read_json(f, orient='records', lines=True)\n",
    "            for f in files), \n",
    "            ignore_index=True\n",
    "            )\n",
    "    if progress == True:\n",
    "            df = pd.concat(\n",
    "            (pd.read_json(f, orient='records', lines=True)\n",
    "            for f in tqdm(files)), \n",
    "            ignore_index=True\n",
    "            )\n",
    "\n",
    "    # If loaded files are aggregated chunks, perform a final aggregation\n",
    "    # to combine any rows across chunks\n",
    "    if aggData == True:\n",
    "        df = df.groupby(group_cols).sum().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_as_chunks(path:str, limit=None, chunksize=2.5e5):\n",
    "    \"\"\"\n",
    "    Load large jsonl file in chunks and then combine to save memory.\n",
    "\n",
    "    :param path: Relative filepath to jsonl file\n",
    "    :param limit: max number of chunks to include in concat. Use to\n",
    "        load a subset of the data\n",
    "    :param chunksize: int size of the lines to include\n",
    "\n",
    "    \"\"\"\n",
    "    # Load jsonl in chunks\n",
    "    df_chunks = pd.read_json(path, orient='records', lines=True, chunksize=chunksize)\n",
    "\n",
    "    # Create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Fill empty dataframe with chunks up to limit\n",
    "    for label, chunk in enumerate(df_chunks):\n",
    "        if label == limit:\n",
    "            break\n",
    "        df = pd.concat([df,chunk], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_chunk_group_save(path:str, \n",
    "                                directory:str, \n",
    "                                start=0, \n",
    "                                stop=None, \n",
    "                                chunksize=2.5e5, \n",
    "                                group_cols:list=['targetFromSourceId', \n",
    "                                                'Gene_symbol', \n",
    "                                                'diseaseFromSourceMappedId', \n",
    "                                                'Disease', \n",
    "                                                'datasourceId']):\n",
    "    \"\"\"\n",
    "    Load large jsonl file in chunks, aggregate, and then save as \n",
    "    smaller jsonl files.  JSONL must already include the following \n",
    "    fields: 'targetFromSourceId', 'Gene_symbol', \n",
    "    'diseaseFromSourceMappedId', 'Disease', 'datasourceId'\n",
    "\n",
    "    :param path: Relative filepath to jsonl file\n",
    "    :param directory: Relative filepath to folder for jsonl chunk files\n",
    "    :param start: index of starting chunk (inclusive)\n",
    "    :param stop: index of maximum chunk number\n",
    "    :param chunksize: int size of the lines to include\n",
    "    :param group_cols: list of columns by which to group raw data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Make versioned directory if does not exist\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Load jsonl in chunks\n",
    "    df_chunks = pd.read_json(path, orient='records', lines=True, chunksize=chunksize)\n",
    "\n",
    "    # Handle each chunk within range limit\n",
    "    for label, chunk in enumerate(df_chunks):\n",
    "        if label >= start:\n",
    "            if label == stop:\n",
    "                break   \n",
    "            \n",
    "            # Group df by only summary columns used for the PCDN and tests\n",
    "            df_grouped = chunk.groupby(group_cols).size().reset_index().rename(columns={0:'evidenceCount'})\n",
    "\n",
    "            # Iterate through JSONL partitions to export as files. Show NaN values as blank    \n",
    "            dict_records = df_grouped.apply(lambda x: x.dropna()).to_dict('records')\n",
    "            jsonlData = ndjson.dumps(dict_records).encode('utf-8')\n",
    "            with open(f\"{directory}part_{label:04}.jsonl\", 'wb') as file:\n",
    "                file.write(jsonlData)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OT files as dataframes\n",
    "ot_diseases = load_jsonl_files_to_df(OT_DISEASES_PATH)\n",
    "ot_targets = load_jsonl_files_to_df(OT_TARGETS_PATH, progress=True)\n",
    "ot_aotf = load_jsonl_files_to_df(OT_AOTF_PATH, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CHoP files as dataframes\n",
    "\n",
    "# Somatic Alterations \n",
    "cnv = pd.read_json(CNV_PATH, orient='records', lines=True)\n",
    "snvGene = pd.read_json(SNVGENE_PATH, orient='records', lines=True)\n",
    "snv = pd.read_json(SNV_PATH, orient='records', lines=True)\n",
    "fusionGene = pd.read_json(FUSIONGENE_PATH, orient='records', lines=True)\n",
    "fusion = pd.read_json(FUSION_PATH, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene Expression\n",
    "tpmGene = load_jsonl_as_chunks(TPMGENE_PATH)\n",
    "tpmGroup = load_jsonl_as_chunks(TPMGROUP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epigenetic Modification: Methylation (raw data)\n",
    "\n",
    "# Load large jsonl, partition and save for faster later uses\n",
    "# Process takes ~7.5min/GB of raw file on local (1.5-2hr) \n",
    "# Only needs to run once for each new data version\n",
    "load_jsonl_chunk_group_save(METHYL_PATH, GROUPED_METHYL_PATH)\n",
    "load_jsonl_chunk_group_save(METHYLGENE_PATH, GROUPED_METHYLGENE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epigenetic Modification: Methylation (grouped data)\n",
    "methyl = load_jsonl_files_to_df(GROUPED_METHYL_PATH, filetype='*.jsonl', progress=True, aggData=True)\n",
    "methylGene = load_jsonl_files_to_df(GROUPED_METHYLGENE_PATH, filetype='*.jsonl', progress=True, aggData=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential Expression (raw data)\n",
    "\n",
    "# Load large jsonl, partition and save for faster later uses\n",
    "# Process takes ~7.5min/GB of raw file on local (~2.5hr) \n",
    "# Only needs to run once for each new data version\n",
    "load_jsonl_chunk_group_save(DIFFEXPR_PATH, \n",
    "                            GROUPED_DIFFEXPR_PATH, \n",
    "                            group_cols=['targetFromSourceId', \n",
    "                                        'Gene_symbol', \n",
    "                                        'diseaseFromSourceMappedId', \n",
    "                                        'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential Expression (grouped data)\n",
    "\n",
    "# Special handling to standardize datasourceId\n",
    "diffExpr = load_jsonl_files_to_df(GROUPED_DIFFEXPR_PATH, \n",
    "                                  filetype='*.jsonl', \n",
    "                                  progress=True, \n",
    "                                  aggData=True, \n",
    "                                  group_cols=['targetFromSourceId', \n",
    "                                                'Gene_symbol', \n",
    "                                                'diseaseFromSourceMappedId', \n",
    "                                                'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PMTL as dataframes\n",
    "pmtl_df = pd.read_json(PMTL_PATH, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of priorities for testing as dataframes\n",
    "priority_targets = pd.read_csv(PRIORITY_TARGETS_PATH)\n",
    "priority_diseases = pd.read_csv(PRIORITY_DISEASES_PATH)\n",
    "priority_evidences = pd.read_csv(PRIORITY_EVIDENCES_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean & Transform Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify OT Target and Disease datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except for ids, names, and disease descendants\n",
    "ot_targets = ot_targets.loc[:, ['id', 'approvedSymbol']]\n",
    "ot_diseases = ot_diseases.loc[:, ['id', 'name', 'descendants']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_OT_table(df:pd.DataFrame, outfile:str, overwrite:bool=False):\n",
    "    \"\"\"\n",
    "    Export CSVs of simplified target and disease tables from OT\n",
    "    for easier ad-hoc analyses.\n",
    "    \n",
    "    :param df: pandas dataframe to export\n",
    "    :param outfile: filename path for export\n",
    "    :param overwrite: replace existing files, default False\n",
    "    \"\"\"\n",
    "\n",
    "    # Skip function if files already exist unless overwrite=True\n",
    "    if os.path.exists(outfile) == True:\n",
    "        if overwrite == False:\n",
    "            return None\n",
    "\n",
    "    # Make output directory if it doesn't exist\n",
    "    if os.path.exists(os.path.dirname(outfile)) == False:\n",
    "        os.makedirs(os.path.dirname(outfile))\n",
    "\n",
    "    # Export df as csv\n",
    "    df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export OT target and disease lists\n",
    "export_OT_table(ot_targets, OT_TARGET_EXPORT_PATH)\n",
    "export_OT_table(ot_diseases, OT_DISEASE_EXPORT_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat CHoP API files for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to rename\n",
    "tpmColRenameDict = {\n",
    "    'Gene_Ensembl_ID': 'targetFromSourceId',\n",
    "    'EFO': 'diseaseFromSourceMappedId',\n",
    "    'cohort': 'Dataset'\n",
    "}\n",
    "\n",
    "# Rename columns and add datasourceId column for each file\n",
    "tpmGene.rename(columns=tpmColRenameDict, inplace=True)\n",
    "tpmGene['datasourceId'] = 'chop_tpm_genewise_expression'\n",
    "\n",
    "tpmGroup.rename(columns=tpmColRenameDict, inplace=True)\n",
    "tpmGroup['datasourceId'] = 'chop_tpm_groupwise_expression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Differential Expression datasourceId values to a standard\n",
    "diffExpr['datasourceId'] = 'chop_differential_expression'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean IDs within CHoP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chop_targets(df:pd.DataFrame, ot_targets:pd.DataFrame=ot_targets, output:bool=True):\n",
    "    \"\"\"\n",
    "    Remove rows of evidence that contain blank or incompatible Target IDs.\n",
    "    These represent values that will not load into the MTP database. \n",
    "    \n",
    "    :param df: Dataframe of CHoP evidence file\n",
    "    :param ot_targets: Dataframe of Open Targets target database\n",
    "    \"\"\"\n",
    "\n",
    "    # Note any rows with blank target IDs\n",
    "    if 'evidenceCount' in df.columns:\n",
    "        blankEvidences = df[df['targetFromSourceId'] == '']['evidenceCount'].sum()\n",
    "    else:\n",
    "        blankEvidences = len(df[df['targetFromSourceId'] == ''])\n",
    "    blankTargets = df[df['targetFromSourceId'] == '']['Gene_symbol'].unique().tolist()\n",
    "\n",
    "    # Drop any rows with blank target IDs\n",
    "    df.drop(df[df['targetFromSourceId'] == ''].index, inplace=True)\n",
    "\n",
    "\n",
    "    # Enrich chop df with OT target ids and symbols\n",
    "    df1 = pd.merge(\n",
    "        df, ot_targets, how='left', left_on='targetFromSourceId', right_on='id').rename(\n",
    "        columns={'id':'otTargetId', 'approvedSymbol':'otSymbol'})\n",
    "\n",
    "    # Note any rows with target IDs not found within OT database\n",
    "    if 'evidenceCount' in df.columns:\n",
    "        invalidEvidences = df1[df1['otTargetId'].isna()]['evidenceCount'].sum()\n",
    "    else:\n",
    "        invalidEvidences = len(df1[df1['otTargetId'].isna()])\n",
    "    invalidTargets = df1[df1['otTargetId'].isna()]['targetFromSourceId'].unique().tolist()\n",
    "\n",
    "\n",
    "    # Drop any rows with target IDs not found within OT database\n",
    "    df2 = df1[df1['otTargetId'].notnull()]\n",
    "\n",
    "    # Printed output\n",
    "    if output == True:\n",
    "        print(f\"    {blankEvidences} evidences across {len(blankTargets)} gene symbols with blank target IDs removed from {df2.datasourceId[0]}\")\n",
    "        if len(blankTargets) > 0:\n",
    "            print('    Gene Symbols of blank IDs:', *blankTargets, sep='\\n        ')\n",
    "        print(f\"    {invalidEvidences} evidences across {len(invalidTargets)} invalid target IDs removed from {df2.datasourceId[0]}\")\n",
    "        if len(invalidTargets) > 0:\n",
    "            print('    Invalid Target IDs:', *invalidTargets, sep='\\n        ')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chop_diseases(df:pd.DataFrame, ot_diseases:pd.DataFrame=ot_diseases, output:bool=True):\n",
    "    \"\"\"\n",
    "    Remove rows of evidence that contain blank or incompatible Disease IDs.\n",
    "    These represent values that will not load into the MTP database. \n",
    "    \n",
    "    :param df: Dataframe of CHoP evidence file\n",
    "    :param ot_diseases: Dataframe of Open Targets disease database\n",
    "    \"\"\"\n",
    "\n",
    "    # Note any rows with blank disease IDs\n",
    "    if 'evidenceCount' in df.columns:\n",
    "        blankEvidences = df[df['diseaseFromSourceMappedId'] == '']['evidenceCount'].sum()\n",
    "    else:\n",
    "        blankEvidences = len(df[df['diseaseFromSourceMappedId'] == ''])\n",
    "    blankDiseases = df[df['diseaseFromSourceMappedId'] == '']['Disease'].unique().tolist()\n",
    "\n",
    "\n",
    "    # Drop any rows with blank disease IDs\n",
    "    df.drop(df[df['diseaseFromSourceMappedId'] == ''].index, inplace=True)\n",
    "\n",
    "\n",
    "    # Enrich chop df with OT disease ids and symbols\n",
    "    df1 = pd.merge(\n",
    "        df, ot_diseases, how='left', left_on='diseaseFromSourceMappedId', right_on='id').rename(\n",
    "        columns={'id':'otDiseaseId', 'name':'otDiseaseName'})\n",
    "\n",
    "    # Note any rows with disease IDs not found within OT database\n",
    "    if 'evidenceCount' in df.columns:\n",
    "        invalidEvidences = df1[df1['otDiseaseId'].isna()]['evidenceCount'].sum()\n",
    "    else:\n",
    "        invalidEvidences = len(df1[df1['otDiseaseId'].isna()])\n",
    "    invalidDiseases = df1[df1['otDiseaseId'].isna()]['diseaseFromSourceMappedId'].unique().tolist()\n",
    "\n",
    "    # Drop any rows with disease IDs not found within OT database\n",
    "    df2 = df1[df1['otDiseaseId'].notnull()]\n",
    "\n",
    "    # Printed output\n",
    "    if output == True:\n",
    "        print(f\"    {blankEvidences} evidences across {len(blankDiseases)} disease names with blank disease IDs removed from {df2.datasourceId[0]}\")\n",
    "        if len(blankDiseases) > 0:\n",
    "            print('    Disease Names of blank IDs:', *blankDiseases, sep='\\n        ')\n",
    "        print(f\"    {invalidEvidences} evidences across {len(invalidDiseases)} invalid disease IDs removed from {df2.datasourceId[0]}\")\n",
    "        if len(invalidDiseases) > 0:\n",
    "            print('    Invalid Disease IDs:', *invalidDiseases, sep='\\n        ')\n",
    "\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_cleaning_functions(df:pd.DataFrame, \n",
    "                            ot_targets:pd.DataFrame=ot_targets, \n",
    "                            ot_diseases:pd.DataFrame=ot_diseases, \n",
    "                            output:bool=False):\n",
    "    \"\"\"\n",
    "    Combines target and disease ID cleaning functions. \n",
    "    Note that because this cleans evidence in series (targets first), it's possible that \n",
    "    an invalid disease that ONLY appears in evidence with an invalid target will not be \n",
    "    reported out. Final data output will still be clean, but report will not catch the \n",
    "    disease ID. \n",
    "\n",
    "    :param df: pandas dataframe of evidence to clean\n",
    "    :param ot_targets: pandas DataFrame of Open Targets targets to use for reference\n",
    "    :param ot_diseases: pandas DataFrame of Open Targets diseases to use for reference\n",
    "    :param output: Boolean determination of whether details of targets/diseases removed\n",
    "                will be printed in addition to total start/end record counts. Default False\n",
    "    \"\"\"\n",
    "\n",
    "    if 'evidenceCount' in df.columns:\n",
    "        print(df.datasourceId[0], '\\n    Start length:', df['evidenceCount'].sum())\n",
    "    else:\n",
    "        print(df.datasourceId[0], '\\n    Start length:', len(df))\n",
    "\n",
    "    df1 = clean_chop_targets(df, output=output)\n",
    "    df2 = clean_chop_diseases(df1, output=output)\n",
    "    \n",
    "    if 'evidenceCount' in df.columns:\n",
    "        print('    End length:', df2['evidenceCount'].sum(), '\\n---')\n",
    "    else:\n",
    "        print('    End length:', len(df2), '\\n---')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each CHoP df through the cleaning functions defined above\n",
    "cnv_clean = chop_cleaning_functions(cnv, output=True)\n",
    "snvGene_clean = chop_cleaning_functions(snvGene, output=True)\n",
    "snv_clean = chop_cleaning_functions(snv, output=True)\n",
    "fusion_clean = chop_cleaning_functions(fusion, output=True)\n",
    "fusionGene_clean = chop_cleaning_functions(fusionGene, output=True)\n",
    "\n",
    "tpmGene_clean = chop_cleaning_functions(tpmGene, output=True)\n",
    "tpmGroup_clean = chop_cleaning_functions(tpmGroup, output=True)\n",
    "\n",
    "methyl_clean = chop_cleaning_functions(methyl, output=True)\n",
    "methylGene_clean = chop_cleaning_functions(methylGene, output=True)\n",
    "\n",
    "diffExpr_clean = chop_cleaning_functions(diffExpr, output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Target-Disease Evidence Dataframe Function\n",
    "Build a dataframe containing all of the pediatric cancer evidence in the format required for validation. Test cases will be subsets of this df, exported into Excel for automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_case_df(dfList:list, ot_targets:pd.DataFrame=ot_targets, ot_diseases:pd.DataFrame=ot_diseases):\n",
    "    \"\"\"\n",
    "    Build and format a dataframe to use when generating target and evidence page tests.\n",
    "    Combine and transform a list of cleaned/preprocessed evidence dataframes.\n",
    "    \n",
    "    :param dfList: list of pandas DataFrames containing evidence\n",
    "    :param ot_targets: pandas DataFrame of Open Targets targets to use for reference\n",
    "    :param ot_diseases: pandas DataFrame of Open Targets diseases to use for reference\n",
    "    \"\"\"\n",
    "\n",
    "    # Create blank output to fill with each evidence df\n",
    "    dfCombined = pd.DataFrame()\n",
    "\n",
    "    # Iterate through list of evidence dataframes\n",
    "    for df in dfList:\n",
    "\n",
    "        # Group data by 5 columns and get evidence count\n",
    "        # All evidence must use identical column names/contents\n",
    "        if 'evidenceCount' in df.columns:\n",
    "            df1 = df.groupby(\n",
    "                ['targetFromSourceId', \n",
    "                'Gene_symbol', \n",
    "                'diseaseFromSourceMappedId', \n",
    "                'Disease', \n",
    "                'datasourceId']\n",
    "                ).sum().reset_index()\n",
    "        else:\n",
    "            df1 = df.groupby(\n",
    "                ['targetFromSourceId', \n",
    "                'Gene_symbol', \n",
    "                'diseaseFromSourceMappedId', \n",
    "                'Disease', \n",
    "                'datasourceId']\n",
    "                ).size().reset_index().rename(columns={0:'evidenceCount'})\n",
    "        \n",
    "    \n",
    "        # Add each formatted df into a single dataframe\n",
    "        dfCombined = pd.concat([dfCombined, df1], ignore_index=True)\n",
    "\n",
    "    # Pivot to organize datasources as columns showing evidence sums\n",
    "    df2 = dfCombined.pivot_table(\n",
    "            values='evidenceCount', \n",
    "            index=['targetFromSourceId','diseaseFromSourceMappedId'], \n",
    "            columns='datasourceId', \n",
    "            aggfunc=sum, fill_value=0\n",
    "            ).reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "    # Use target IDs to map canonical OT names for targets\n",
    "    df3 = pd.merge(df2, ot_targets, how='left', left_on='targetFromSourceId', right_on='id').rename(columns={'approvedSymbol':'targetNameOT'})\n",
    "    df3.drop(columns='id', inplace=True)\n",
    "\n",
    "    # Use disease IDs to map canonical OT names for diseases\n",
    "    df4 = pd.merge(df3, ot_diseases, how='left', left_on='diseaseFromSourceMappedId', right_on='id').rename(columns={'name':'diseaseNameOT'})\n",
    "    df4.drop(columns='id', inplace=True)\n",
    "\n",
    "    # Rearrange output columns\n",
    "    df5 = df4[[\n",
    "                'targetFromSourceId',\n",
    "                'diseaseFromSourceMappedId',\n",
    "                'targetNameOT',\n",
    "                'diseaseNameOT',\n",
    "                'descendants',\n",
    "                'chop_gene_level_snv',\n",
    "                'chop_variant_level_snv',\n",
    "                'chop_gene_level_cnv',\n",
    "                'chop_putative_oncogene_fused_gene',\n",
    "                'chop_putative_oncogene_fusion',\n",
    "                'chop_tpm_groupwise_expression',\n",
    "                'chop_tpm_genewise_expression',\n",
    "                'chop_gene_level_methylation',\n",
    "                'chop_isoform_level_methylation',\n",
    "                'chop_differential_expression',]]\n",
    "\n",
    "    return df5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function to combine evidence and build test case dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of clean dataframes for iteration\n",
    "dfList = [\n",
    "    cnv_clean,\n",
    "    snvGene_clean,\n",
    "    snv_clean,\n",
    "    fusion_clean,\n",
    "    fusionGene_clean,\n",
    "    tpmGene_clean,\n",
    "    tpmGroup_clean,\n",
    "    methylGene_clean,\n",
    "    methyl_clean,\n",
    "    diffExpr_clean,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test case\n",
    "testCase_df = build_test_case_df(dfList, ot_targets, ot_diseases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and export Pediatric Cancer Data Navigation table using the Test Case Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pcdn_df(df:pd.DataFrame=testCase_df):\n",
    "    \"\"\"\n",
    "    Build the Pediatric Cancer Data Navigation table\n",
    "    by reformatting the test case dataframe. \n",
    "    \n",
    "    :param df: pandas dataFrame of combined and processed evidence\n",
    "    \"\"\"\n",
    "\n",
    "    # Define labels and source columns for True/False summary columns\n",
    "    # This assumes that gene-level evidence presence matches variant-level\n",
    "    # presence in relevant data sources\n",
    "    pcdnSummaryDict = {\n",
    "    'SNV': 'chop_gene_level_snv',\n",
    "    'CNV': 'chop_gene_level_cnv',\n",
    "    'Fusion': 'chop_putative_oncogene_fused_gene',\n",
    "    'GeneExpression': 'chop_tpm_genewise_expression',\n",
    "    'Methylation': 'chop_gene_level_methylation',\n",
    "    'DifferentialExpression': 'chop_differential_expression'\n",
    "    }\n",
    "\n",
    "    # Rename columns for clarity and to match input format\n",
    "    df1 = df.rename(columns={'targetNameOT':'Gene_symbol', 'diseaseNameOT':'Disease'})\n",
    "\n",
    "    # Create summary True/False columns for evidence presence\n",
    "    for label, col in pcdnSummaryDict.items():\n",
    "        df1[label] = np.where(df1[col] > 0, True, False)\n",
    "\n",
    "    # Create id column with uuids\n",
    "    df1['id'] = df1.apply(lambda x: uuid.uuid4(), axis=1)\n",
    "\n",
    "    # Keep only columns of interest\n",
    "    df2 = df1[\n",
    "        ['targetFromSourceId',\n",
    "        'diseaseFromSourceMappedId',\n",
    "        'Gene_symbol',\n",
    "        'Disease',\n",
    "        'SNV',\n",
    "        'CNV',\n",
    "        'Fusion',\n",
    "        'GeneExpression',\n",
    "        'Methylation',\n",
    "        'DifferentialExpression',\n",
    "        'id']]\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df_as_json(df:pd.DataFrame, outfile:str):\n",
    "    \"\"\"\n",
    "    Export pandas dataframe as JSON file. Parsing using\n",
    "    json.dumps is needed to avoid extra backslashes otherwise\n",
    "    present if using only built-in pandas to_json().\n",
    "    \n",
    "    :param df: pandas dataframe to export\n",
    "    :param outfile: filename path for export\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make output directory if it doesn't exist\n",
    "    if os.path.exists(os.path.dirname(outfile)) == False:\n",
    "        os.makedirs(os.path.dirname(outfile))\n",
    "\n",
    "    # Load df as json string and parse\n",
    "    # Use default_handler to avoid overflow error with uuids\n",
    "    json_str = df.to_json(orient='records', default_handler=str)\n",
    "    parsed = json.loads(json_str)\n",
    "\n",
    "    # Export to json filepath\n",
    "    with open(outfile, 'w') as json_file:\n",
    "        json_file.write(json.dumps(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df_as_jsonl_chunks(df:pd.DataFrame, \n",
    "                                directory:str, \n",
    "                                maxlines:int, \n",
    "                                fileformat:str):\n",
    "    \"\"\"\n",
    "    Export pandas dataframe as many JSONL chunks for easier ETL.\n",
    "\n",
    "    :param df: pandas dataframe to export\n",
    "    :param directory: filepath of directory to hold exported jsonl\n",
    "    :param maxlines: int max number of jsonl lines per file\n",
    "    :param fileformat: name format for export files. \n",
    "        Must contain '{}' string for part number formatting\n",
    "    \"\"\"\n",
    "\n",
    "    # Make versioned directory if does not exist\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Load pandas df as json lines object\n",
    "    # Use default_handler to avoid overflow error with uuids\n",
    "    json_str = df.to_json(orient='records', default_handler=str, lines=True)\n",
    "\n",
    "    # Load json lines as chunks with max lines of 50K per file\n",
    "    chunks = pd.read_json(json_str, orient='records', lines=True, chunksize=maxlines)\n",
    "\n",
    "    # Iterate through JSONL partitions to export as files. Show NaN values as blank\n",
    "    for label, chunk in enumerate(chunks):\n",
    "        dict_records = chunk.apply(lambda x: x.dropna()).to_dict('records')\n",
    "        jsonlData = ndjson.dumps(dict_records).encode('utf-8')\n",
    "        with open(directory+fileformat.format(label), 'wb') as file:\n",
    "            file.write(jsonlData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_md5(filename:str, suffix:str='_md5.txt', compression_type:str='zip'):\n",
    "    \"\"\"\n",
    "    Create txt file containing md5sum of file. \n",
    "    \n",
    "    :param filename: path of file to compress\n",
    "    :param suffix: string to attach to end of md5.txt file\n",
    "    :param compression_type: file extension of filename\n",
    "    \"\"\"\n",
    "\n",
    "    compression_str = '.'+compression_type\n",
    "\n",
    "    with open(filename+compression_str, 'rb') as f:\n",
    "        contents = f.read() \n",
    "        md5_returned = hashlib.md5(contents).hexdigest()\n",
    "\n",
    "    with open(filename+suffix, 'w') as file:\n",
    "        file.write(md5_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_temp_folder(directory:str):\n",
    "    \"\"\"\n",
    "    Delete directory and all contents.\n",
    "\n",
    "    :param directory: path of directory to delete\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(directory)\n",
    "    except OSError as error:\n",
    "        print(f'Error: {directory}: {error.strerror}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_directory(filename, root, compression_type='zip'):\n",
    "    \"\"\"\n",
    "    Compress and zip a file directory.\n",
    "    \n",
    "    :param filename: name of the directory to compress. This will\n",
    "                        also be used as filename for the output\n",
    "    :param root: parent directory containing the directory to compress\n",
    "    :param compression_type: compression type string, default 'zip'\n",
    "    \"\"\"\n",
    "\n",
    "    shutil.make_archive(filename, compression_type, root, filename.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_jsonl(df:pd.DataFrame,\n",
    "                    directory:str,\n",
    "                    maxlines:int,\n",
    "                    fileformat:str,\n",
    "                    filename:str,\n",
    "                    root:str,\n",
    "                    compression_type:str='zip',\n",
    "                    suffix:str='_md5.txt'):\n",
    "    \"\"\"\n",
    "    Package a pandas dataframe into multiple JSONL files, compress,\n",
    "    and then get md5 checksum. \n",
    "    \n",
    "    :param df: pandas dataframe to export\n",
    "    :param directory: filepath of directory to hold exported jsonl\n",
    "    :param maxlines: int max number of jsonl lines per file\n",
    "    :param fileformat: name format for export files\n",
    "                    Must contain '{}' string for part number formatting\n",
    "    :param filename: name of the directory to compress. This will\n",
    "                    also be used as filename for the output\n",
    "    :param root: parent directory containing the directory to compress\n",
    "    :param compression_type: compression type string, default 'zip'\n",
    "    :param suffix: string to attach to end of md5.txt file\n",
    "    \"\"\"\n",
    "\n",
    "    export_df_as_jsonl_chunks(df, directory, maxlines, fileformat)\n",
    "    compress_directory(filename, root, compression_type=compression_type)\n",
    "    output_md5(filename, suffix=suffix, compression_type=compression_type)\n",
    "    clear_temp_folder(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid PCDN\n",
    "pcdn_df = build_pcdn_df(testCase_df)\n",
    "\n",
    "# Export as single JSON\n",
    "export_df_as_json(pcdn_df, PCDN_JSON)\n",
    "\n",
    "# Export as multiple JSONL\n",
    "package_jsonl(df=pcdn_df, \n",
    "            directory=PCDN_JSONL_TEMP,\n",
    "            maxlines=3000,\n",
    "            fileformat=PCDN_JSONL_FILENAMES,\n",
    "            filename=PCDN_JSONL,\n",
    "            root=PCDN_PATH,\n",
    "            compression_type='zip',\n",
    "            suffix='_md5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcdn_counts(pcdn_df:pd.DataFrame=pcdn_df, outfile:str=PCDN_COUNTS):\n",
    "    \"\"\"\n",
    "    Export PCDN summary counts for PCDN page header\n",
    "    \n",
    "    :param pcdn_df: pandas dataframe of PCDN\n",
    "    :param outfile: output filename\n",
    "    \"\"\"\n",
    "    # Build dict with summary counts\n",
    "    counts = {\n",
    "        'Targets': pcdn_df['Gene_symbol'].nunique(),\n",
    "        'Diseases': pcdn_df['Disease'].nunique(),\n",
    "        'Evidences': len(pcdn_df)\n",
    "        }\n",
    "    \n",
    "    # Recast as json object and export\n",
    "    counts_obj = json.dumps(counts)\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(counts_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcdn_diseases(pcdn_df:pd.DataFrame=pcdn_df, outfile:str=PCDN_DISEASES):\n",
    "    \"\"\"\n",
    "    Build and export list of unique (OT) disease names within the PCDN.\n",
    "    This can be used for the PCDN drop-down menu.\n",
    "    \n",
    "    :param pcdn_df: pandas dataframe of the PCDN\n",
    "    :param outfile: output filename\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique Diseases and IDs from the PCDN\n",
    "    df = pcdn_df.groupby(['Disease','diseaseFromSourceMappedId']).size().reset_index()[['Disease', 'diseaseFromSourceMappedId']]\n",
    "\n",
    "    # Sort by disease name, ignoring case\n",
    "    df.sort_values(by='Disease', inplace=True, key=lambda col: col.str.lower())\n",
    "\n",
    "    # Load df as json string and parse\n",
    "    # Use default_handler to avoid overflow error with uuids\n",
    "    json_str = df.to_json(orient='records', default_handler=str)\n",
    "    parsed = json.loads(json_str)\n",
    "\n",
    "    # Export to json filepath\n",
    "    with open(outfile, 'w') as json_file:\n",
    "        json_file.write(json.dumps(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export PCDN summary counts\n",
    "get_pcdn_counts(pcdn_df, PCDN_COUNTS)\n",
    "\n",
    "# Export PCDN disease list for drop-down menu\n",
    "get_pcdn_diseases(pcdn_df, PCDN_DISEASES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate priority test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_priority_tests(priority_diseases:pd.DataFrame=priority_diseases,\n",
    "                            priority_targets:pd.DataFrame=priority_targets,\n",
    "                            priority_evidences:pd.DataFrame=priority_evidences,\n",
    "                            ot_diseases:pd.DataFrame=ot_diseases,\n",
    "                            ot_targets:pd.DataFrame=ot_targets):\n",
    "    \"\"\"\n",
    "    Validate that all priority targets and diseases defined in input priority csvs\n",
    "    are found within OT's target and disease databases before continuing.\n",
    "\n",
    "    :param priority_diseases: pandas dataframe of curated diseases to include in test cases\n",
    "    :param priority_targets: pandas dataframe of curated targets to include in test cases\n",
    "    :param priority_evidences: pandas dataframe of curated target-disease combinations to include in test cases\n",
    "    :param ot_diseases: pandas dataframe of Open Targets disease database\n",
    "    :param ot_targets: pandas dataframe of Open Targets target database\n",
    "    \"\"\"\n",
    "    \n",
    "    invalidDiseases = priority_diseases[~priority_diseases['diseaseId'].isin(ot_diseases['id'])]\n",
    "    if len(invalidDiseases) > 0: print(f\"DISEASE.CSV: \\n {invalidDiseases} \\n ---\")\n",
    "\n",
    "    invalidTargets = priority_targets[~priority_targets['targetId'].isin(ot_targets['id'])]\n",
    "    if len(invalidTargets) > 0: print(f\"TARGETS.CSV: \\n {invalidTargets} \\n ---\")\n",
    "\n",
    "    invalidEvidenceDiseases = priority_evidences[~priority_evidences['diseaseId'].isin(ot_diseases['id'])]\n",
    "    if len(invalidEvidenceDiseases) > 0: print(f\"EVIDENCES.CSV: \\n {invalidEvidenceDiseases} \\n ---\")\n",
    "\n",
    "    invalidEvidenceTargets = priority_evidences[~priority_evidences['targetId'].isin(ot_targets['id'])]\n",
    "    if len(invalidEvidenceTargets) > 0: print(f\"EVIDENCES.CSV: \\n {invalidEvidenceTargets} \\n ---\")\n",
    "\n",
    "    assert all(\n",
    "                [len(invalidDiseases) == 0,\n",
    "                len(invalidTargets) == 0,\n",
    "                len(invalidEvidenceDiseases) == 0,\n",
    "                len(invalidEvidenceTargets) == 0\n",
    "                ]),f\"Priority test case csvs have invalid target and/or disease ids.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_priority_test_relevance(priority_diseases:pd.DataFrame=priority_diseases,\n",
    "                                    priority_targets:pd.DataFrame=priority_targets,\n",
    "                                    priority_evidences:pd.DataFrame=priority_evidences,\n",
    "                                    testCase_df:pd.DataFrame=testCase_df):\n",
    "    \"\"\"\n",
    "    Validate that all priority targets and diseases defined in input priority csvs\n",
    "    are found within at least one pediatric dataset.\n",
    "    \n",
    "    :param priority_diseases: pandas dataframe of curated diseases to include in test cases\n",
    "    :param priority_targets: pandas dataframe of curated targets to include in test cases\n",
    "    :param priority_evidences: pandas dataframe of curated target-disease combinations to include in test cases\n",
    "    :param testCase_df: pandas DataFrame of preprocessed evidence data\n",
    "    \"\"\"\n",
    "    \n",
    "    pedTargets = testCase_df['targetFromSourceId'].unique().tolist()\n",
    "    pedDiseases = testCase_df['diseaseFromSourceMappedId'].unique().tolist()\n",
    "    \n",
    "    invalidDiseases = priority_diseases[~priority_diseases['diseaseId'].isin(pedDiseases)]\n",
    "    if len(invalidDiseases) > 0: print(f\"DISEASE.CSV: \\n {invalidDiseases} \\n ---\")\n",
    "\n",
    "    invalidTargets = priority_targets[~priority_targets['targetId'].isin(pedTargets)]\n",
    "    if len(invalidTargets) > 0: print(f\"TARGETS.CSV: \\n {invalidTargets} \\n ---\")\n",
    "\n",
    "    invalidEvidenceDiseases = priority_evidences[~priority_evidences['diseaseId'].isin(pedDiseases)]\n",
    "    if len(invalidEvidenceDiseases) > 0: print(f\"EVIDENCES.CSV: \\n {invalidEvidenceDiseases} \\n ---\")\n",
    "\n",
    "    invalidEvidenceTargets = priority_evidences[~priority_evidences['targetId'].isin(pedTargets)]\n",
    "    if len(invalidEvidenceTargets) > 0: print(f\"EVIDENCES.CSV: \\n {invalidEvidenceTargets} \\n ---\")\n",
    "\n",
    "    assert all(\n",
    "                [len(invalidDiseases) == 0,\n",
    "                len(invalidTargets) == 0,\n",
    "                len(invalidEvidenceDiseases) == 0,\n",
    "                len(invalidEvidenceTargets) == 0\n",
    "                ]),f\"Priority test case csvs have target and/or disease ids not found in pediatric data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run priority test case validations\n",
    "validate_priority_tests()\n",
    "check_priority_test_relevance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Test Case Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Target Associations (Direct Overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_targetAssc(sampleSize:int,\n",
    "                            ot_aotf:pd.DataFrame=ot_aotf, \n",
    "                            ot_targets:pd.DataFrame=ot_targets, \n",
    "                            pmtl_df:pd.DataFrame=pmtl_df, \n",
    "                            priority_targets:pd.DataFrame=priority_targets,\n",
    "                            randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for Target Associations page. Not specific to new MTP data. \n",
    "    Note that until pediatric data is included in association scoring, MTP associations will\n",
    "    be identical to OT associations. UPDATED function uses OT Associations on the Fly\n",
    "\n",
    "    :param sampleSize: int number of random targets to include in test\n",
    "    :param ot_aotf: pandas DataFrame of OpenTargets Associations on the Fly (AOTF)\n",
    "    :param ot_targets: pandas DataFrame of OpenTargets target names for reference\n",
    "    :param pmtl_df: pandas DataFrame of FDA PMTL\n",
    "    :param priority_targets: pandas DataFrame of targets to always include in test case\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    # Group associations by target and disease ID, \n",
    "    # then again by only target Id to mimic associations heatmap\n",
    "    df = ot_aotf.groupby(['target_id', 'disease_id']).size().reset_index(\n",
    "        ).groupby('target_id').size().reset_index()\n",
    "\n",
    "    # Enrich associations with target names (OT Targets) and PMTL designations\n",
    "    df1 = pd.merge(df, ot_targets, how='left', left_on='target_id', right_on='id').merge(\n",
    "                    pmtl_df[['ensemblID', 'designation']], \n",
    "                    how='left', left_on='target_id', right_on='ensemblID')\n",
    "\n",
    "    # Rename columns\n",
    "    df2 = df1.rename(columns={\n",
    "        0:'diseaseCount',\n",
    "        'approvedSymbol':'targetNameOT',\n",
    "        'designation':'PMTLcode'})\n",
    "        \n",
    "    # Recast NaN PMTL as Unspecified\n",
    "    df2.fillna('Unspecified Target', inplace=True)\n",
    "\n",
    "    # Add suffix column\n",
    "    df2['suffixUrl'] = '/target/'+df2['target_id']+'/associations'\n",
    "\n",
    "    # Reorder columns and omit redundant\n",
    "    df3 = df2[\n",
    "                ['suffixUrl', \n",
    "                'target_id', \n",
    "                'targetNameOT', \n",
    "                'PMTLcode',\n",
    "                'diseaseCount']\n",
    "                ]\n",
    "\n",
    "    # Create subset df with priority targets and random sample of other targets\n",
    "    random.seed(randomSeed)\n",
    "    df4 = df3[\n",
    "            (df3['target_id'].isin(priority_targets['targetId'])) |\n",
    "            (df3['target_id'].isin(random.sample(df3['target_id'].unique(\n",
    "            ).tolist(), sampleSize)))]\n",
    "\n",
    "    return df4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Target Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_targetProfile(sampleSize:int, \n",
    "                                testCase_df:pd.DataFrame=testCase_df, \n",
    "                                priority_targets:pd.DataFrame=priority_targets,\n",
    "                                randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for Target Profile page. Should only include targets with\n",
    "    at least some new pediatric data. \n",
    "\n",
    "    :param sampleSize: int number of random targets to include in test case\n",
    "    :param testCase_df: pandas DataFrame of preprocessed evidence data\n",
    "    :param priority_targets: pandas DataFrame of targets to always include in test case\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    df = testCase_df.groupby(['targetFromSourceId','targetNameOT']).sum().reset_index()\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Gene Expression Widget (groupwise plot)\n",
    "    df['opcGeneExp_target'] = np.where(df['chop_tpm_groupwise_expression'] >0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Differential Expression Widget (Target vs all diseases plot)\n",
    "    df['opcDiffExpr_target'] = np.where(df['chop_differential_expression'] >0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # List all datasourceIds to be grouped and considered Somatic Alterations with tab labels\n",
    "    somaticAltCols = {\n",
    "    'chop_gene_level_cnv':'cnvByGene',\n",
    "    'chop_gene_level_snv':'snvByGene',\n",
    "    'chop_putative_oncogene_fused_gene':'fusionByGene',\n",
    "    'chop_putative_oncogene_fusion':'fusion',\n",
    "    'chop_variant_level_snv':'snvByVariant'}\n",
    "\n",
    "    # List all datasourceIds to be grouped and considered Epigenetic Modifications\n",
    "    epigeneticModCols = {\n",
    "    'chop_gene_level_methylation':'methylByGene',\n",
    "    'chop_isoform_level_methylation':'methylByIsoform'}\n",
    "\n",
    "    # Raname columns\n",
    "    df.rename(columns=somaticAltCols, inplace=True)\n",
    "    df.rename(columns=epigeneticModCols, inplace=True)\n",
    "    df.rename(columns={'targetFromSourceId':'targetId'}, inplace=True)\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Somatic Alterations widget\n",
    "    df['opcSomaticAlt'] = np.where(df[somaticAltCols.values()].sum(axis=1) > 0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Epigenetic Modifications widget\n",
    "    df['opcEpiMod'] = np.where(df[epigeneticModCols.values()].sum(axis=1) > 0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Add suffix column\n",
    "    df['suffixUrl'] ='/target/'+df['targetId']\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df1 = df[\n",
    "                ['suffixUrl', \n",
    "                'targetId', \n",
    "                'targetNameOT', \n",
    "                'opcGeneExp_target', \n",
    "                'opcSomaticAlt',\n",
    "                'opcEpiMod',\n",
    "                'opcDiffExpr_target', \n",
    "                'snvByGene', \n",
    "                'snvByVariant', \n",
    "                'cnvByGene', \n",
    "                'fusionByGene', \n",
    "                'fusion',\n",
    "                'methylByGene',\n",
    "                'methylByIsoform']\n",
    "                ]\n",
    "\n",
    "    # Create subset df with priority targets and  random sample of other targets\n",
    "    random.seed(randomSeed)\n",
    "    df2 = df1[\n",
    "            (df1['targetId'].isin(priority_targets['targetId'])) |\n",
    "            (df1['targetId'].isin(random.sample(df1['targetId'].unique().tolist(), sampleSize)))]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Indirect Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indirect_search(df:pd.DataFrame, \n",
    "                    diseaseId:str, \n",
    "                    nonPropDS:list=['expression_atlas'], \n",
    "                    ot_diseases:pd.DataFrame=ot_diseases, \n",
    "                    targetId:str=None,\n",
    "                    diseaseCol:str='disease_id',\n",
    "                    targetCol:str='target_id'):\n",
    "    \"\"\"\n",
    "    Search a dataframe and return all direct and indirect evidence of a disease.\n",
    "    Indirect evidence includes both the target and disease of interest as well as \n",
    "    the target and descendant diseases of interest. \n",
    "\n",
    "    :param df: dataframe to search. Must contain descendants and disease_id cols\n",
    "    :diseaseId: EFO id of disease of interest. \n",
    "    :nonPropDS: Non-Propagating DataSources. List of datasource IDs to exclude from\n",
    "                indirect evidence propagation. Direct evidence for the disease of \n",
    "                interest will be gathered, but no indirect evidence. \n",
    "    :ot_diseases: dataframe of OpenTargets disease database. Used as a backup to get\n",
    "                descendants for a disease that does not appear in direct evidence\n",
    "    :targetId: ENSG id of target of interest. Optional. Default=None\n",
    "    :diseaseCol: Column header for disease IDs. Default 'disease_id' to match AOTF\n",
    "    :targetCol: Column header for target IDs. Default 'target_id' to match AOTF\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Get direct evidence for disease of interest\n",
    "    # Indirect evidence will be iteratively added to this df later\n",
    "    df_combined = df[df[diseaseCol] == diseaseId]\n",
    "\n",
    "    # If no direct evidence found, then use backup disease db\n",
    "    # to get list of descendants for disease of interest\n",
    "    if len(df_combined) == 0:\n",
    "        descList = (ot_diseases[\n",
    "                        ot_diseases['id'] == diseaseId]\n",
    "                        ['descendants'].tolist()[0])\n",
    "    else:\n",
    "        # Get list of descendants for disease of interest\n",
    "        descList = df_combined['descendants'].tolist()[0]\n",
    "\n",
    "    # Loop through descendants and add evidence to the df\n",
    "    for efo in descList:\n",
    "        \n",
    "        # If datasource_id is included, exclude nonProp datasources\n",
    "        if 'datasource_id' in df.columns:\n",
    "            df_temp = df[\n",
    "                        (df[diseaseCol] == efo) & \n",
    "                        (~df['datasource_id'].isin(nonPropDS))]\n",
    "        else: \n",
    "            df_temp = df[\n",
    "                        (df[diseaseCol] == efo)]\n",
    "\n",
    "        df_combined = pd.concat([df_combined, df_temp])\n",
    "    \n",
    "    # If target of interest specified, add final filtering\n",
    "    if targetId != None:\n",
    "        df_combined = df_combined[df_combined[targetCol] == targetId]\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Disease Associations (Indirect Overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_diseaseAssc(sampleSize:int,\n",
    "                            ot_aotf:pd.DataFrame=ot_aotf, \n",
    "                            ot_diseases:pd.DataFrame=ot_diseases,\n",
    "                            priority_diseases:pd.DataFrame=priority_diseases,\n",
    "                            all_ped_diseases:bool=False,\n",
    "                            testCase_df:pd.DataFrame=testCase_df,\n",
    "                            randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for Target Associations page. Not specific to new MTP data. \n",
    "    Note that until pediatric data is included in association scoring, MTP associations will\n",
    "    be identical to OT associations. UPDATED function finds indirect associations using \n",
    "    OT's AOTF files and OT's disease ontology descendants.\n",
    "\n",
    "    :param sampleSize: int number of random diseases to include in test\n",
    "    :param ot_aotf: pandas DataFrame of OpenTargets Associations on the Fly evidence\n",
    "    :param ot_diseases: pandas DataFrame of OpenTargets disease names for reference\n",
    "    :param priority_diseases: pandas DataFrame of diseases to always include in test case\n",
    "    :param all_ped_diseases: if set to True, use all ped diseases instead of priority diseases\n",
    "    :param testCase_df: pandas DataFrame of preprocessed evidence data. Required if all_ped_diseases=True\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of all unique diseases in df\n",
    "    diseaseList = ot_aotf['disease_id'].unique().tolist()\n",
    "\n",
    "    # Initiate random seed for sampling\n",
    "    # Note that there's no logic to prevent overlap between priority and random \n",
    "    # sample lists, but that won't causes errors\n",
    "    random.seed(randomSeed)\n",
    "\n",
    "    # If all_ped_diseases toggled, Get list of all pediatric diseases and add to random sample\n",
    "    if all_ped_diseases == True:\n",
    "        diseaseTestList = (testCase_df['diseaseFromSourceMappedId'].unique().tolist()\n",
    "                           + random.sample(diseaseList, sampleSize))\n",
    "\n",
    "    # If all_ped_diseases not toggled (default False), use priority diseases and add random sample\n",
    "    else:\n",
    "        diseaseTestList = (priority_diseases['diseaseId'].tolist()\n",
    "                            + random.sample(diseaseList, sampleSize))\n",
    "\n",
    "    # Enrich evidence df with disease descendants\n",
    "    df = ot_aotf.merge(ot_diseases, how='left', left_on='disease_id', right_on='id')\n",
    "\n",
    "    # Build df with test case diseases from OT disease database\n",
    "    df1 = (ot_diseases\n",
    "                .loc[ot_diseases['id'].isin(diseaseTestList)]\n",
    "                .reset_index(drop=True)\n",
    "                .rename(columns={'id':'diseaseId','name':'diseaseNameOT'}))\n",
    "\n",
    "    # For each test case disease, run indirect_search function and get count of unique target IDs.\n",
    "    # indirect_search function gathers all evidence for the disease and all descendants\n",
    "    # Runtime for this step varies depending on number of descendants, but usually <30 sec/disease\n",
    "    df1['targetCount'] = df1['diseaseId'].apply(lambda x: indirect_search(df, x)['target_id'].nunique())\n",
    "\n",
    "    # Add suffix column\n",
    "    df1['suffixUrl'] = '/disease/'+df1['diseaseId']+'/associations'\n",
    "\n",
    "    # Reorder columns and omit redundant\n",
    "    df2 = df1[\n",
    "                ['suffixUrl', \n",
    "                'diseaseId', \n",
    "                'diseaseNameOT', \n",
    "                'targetCount']\n",
    "                ]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Disease Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_diseaseProfile(sampleSize:int, \n",
    "                                testCase_df:pd.DataFrame=testCase_df, \n",
    "                                priority_diseases:pd.DataFrame=priority_diseases,\n",
    "                                all_ped_diseases:bool=False,\n",
    "                                randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for Disease Profile page. Should only include diseases with\n",
    "    at least some new pediatric data. \n",
    "\n",
    "    :param sampleSize: int number of random diseases to include in test case\n",
    "    :param testCase_df: pandas DataFrame of preprocessed evidence data\n",
    "    :param priority_diseases: pandas DataFrame of diseases to always include in test case\n",
    "    :param all_ped_diseases: if set to True, use all ped diseases instead of priority diseases\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    df = testCase_df.groupby(['diseaseFromSourceMappedId','diseaseNameOT']).sum().reset_index()\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Differential Expression Widget (Disease vs top targets plot)\n",
    "    df['opcDiffExpr_disease'] = np.where(df['chop_differential_expression'] >0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'diseaseFromSourceMappedId':'diseaseId'}, inplace=True)\n",
    "\n",
    "    # Add suffix column\n",
    "    df['suffixUrl'] ='/disease/'+df['diseaseId']\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df1 = df[\n",
    "                ['suffixUrl', \n",
    "                'diseaseId', \n",
    "                'diseaseNameOT', \n",
    "                'opcDiffExpr_disease']\n",
    "                ]\n",
    "\n",
    "    # If all_ped_diseases toggled, use entire df as-is\n",
    "    if all_ped_diseases == True:\n",
    "        df2 = df1\n",
    "\n",
    "    # If all_ped_diseases not toggled (default False), then create subset df\n",
    "    # with priority diseases and random sample of other ped diseases\n",
    "    # Note that there's no logic to prevent overlap between priority and random \n",
    "    # sample lists, but that won't causes errors\n",
    "    if all_ped_diseases == False:\n",
    "        random.seed(randomSeed)\n",
    "        df2 = df1[\n",
    "                (df1['diseaseId'].isin(priority_targets['diseaseId'])) |\n",
    "                (df1['diseaseId'].isin(random.sample(df1['diseaseId'].unique().tolist(), sampleSize)))]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Evidence Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_evidence(sampleSize:int, \n",
    "                            testCase_df:pd.DataFrame=testCase_df, \n",
    "                            priority_evidences:pd.DataFrame=priority_evidences,\n",
    "                            randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for Evidence page. Should only include target-disease\n",
    "    evidence with at least some new pediatric data. UPDATED function includes \n",
    "    indirect evidence\n",
    "\n",
    "    :param sampleSize: int number of random evidence combinations to include in test case\n",
    "    :param testCase_df: pandas DataFrame of preprocessed evidence data\n",
    "    :param priority_evidences: pandas DataFrame of evudebces to always include in test case\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of target-disease evidence combos from priority list\n",
    "    priorityTuples = (priority_evidences\n",
    "                        [['diseaseId','targetId']]\n",
    "                        .apply(tuple, axis=1).tolist())\n",
    "\n",
    "    # Get random sample of target-disease combos from evidence df\n",
    "    randomTuples = (testCase_df.sample(n=sampleSize, random_state=randomSeed)\n",
    "                        [['diseaseFromSourceMappedId', 'targetFromSourceId']]\n",
    "                        .apply(tuple, axis=1).tolist())\n",
    "\n",
    "    # Combine target-disease combo lists\n",
    "    evidenceTuples = priorityTuples + randomTuples\n",
    "\n",
    "\n",
    "    # List all datasourceIds to be grouped and considered Somatic Alterations with tab labels\n",
    "    somaticAltCols = {\n",
    "                    'chop_gene_level_cnv':'cnvByGene',\n",
    "                    'chop_gene_level_snv':'snvByGene',\n",
    "                    'chop_putative_oncogene_fused_gene':'fusionByGene',\n",
    "                    'chop_putative_oncogene_fusion':'fusion',\n",
    "                    'chop_variant_level_snv':'snvByVariant'\n",
    "                    }\n",
    "\n",
    "    # List all datasourceIds to be grouped and considered Epigenetic Modifications\n",
    "    epigeneticModCols = {\n",
    "                    'chop_gene_level_methylation':'methylByGene',\n",
    "                    'chop_isoform_level_methylation':'methylByIsoform'\n",
    "                    }\n",
    "\n",
    "\n",
    "    # CHoP datasource columns to sum for total indirect evidence\n",
    "    sumCols =      [\n",
    "                    'chop_gene_level_snv', \n",
    "                    'chop_variant_level_snv', \n",
    "                    'chop_gene_level_cnv', \n",
    "                    'chop_putative_oncogene_fused_gene', \n",
    "                    'chop_putative_oncogene_fusion',\n",
    "                    'chop_gene_level_methylation',\n",
    "                    'chop_isoform_level_methylation'\n",
    "                    ]\n",
    "\n",
    "    # CHoP columns to retain as-is for direct evidence\n",
    "    retainCols =    [\n",
    "                    'targetFromSourceId', \n",
    "                    'diseaseFromSourceMappedId', \n",
    "                    'targetNameOT', \n",
    "                    'diseaseNameOT', \n",
    "                    'chop_tpm_groupwise_expression', \n",
    "                    'chop_tpm_genewise_expression'\n",
    "                    ] \n",
    "\n",
    "\n",
    "    # Define empty dataframe to fill\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through evidence target-disease combos to get indirect evidence\n",
    "    for disease, target in evidenceTuples:\n",
    "\n",
    "        # Gather all indirect evidence for a single target-disease combo\n",
    "        df_temp1 = indirect_search(df=testCase_df, \n",
    "                                    diseaseId=disease, \n",
    "                                    targetId=target, \n",
    "                                    diseaseCol='diseaseFromSourceMappedId', \n",
    "                                    targetCol='targetFromSourceId')\n",
    "\n",
    "        # Group by target and sum evidence counts\n",
    "        df_temp2 = df_temp1.groupby('targetFromSourceId')[sumCols].sum().reset_index()\n",
    "\n",
    "        # Join target/evidenceCount df with retained columns to get summed row with all info\n",
    "        df_temp3 = pd.merge(df_temp2, df_temp1[retainCols], how='left', on='targetFromSourceId')\n",
    "\n",
    "        # Select single row with summed indirect evidence counts for target-disese combo\n",
    "        df_temp4 = df_temp3[df_temp3['diseaseFromSourceMappedId'] == disease]\n",
    "        \n",
    "        # Add to cumulative output df\n",
    "        df = pd.concat([df, df_temp4])\n",
    "\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Gene Expression Widget (genewise plot)\n",
    "    df['opcGeneExp_evidence'] = np.where(df['chop_tpm_genewise_expression'] >0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns=somaticAltCols, inplace=True)\n",
    "    df.rename(columns=epigeneticModCols, inplace=True)\n",
    "    df.rename(columns={'targetFromSourceId':'targetId', 'diseaseFromSourceMappedId':'diseaseId'}, inplace=True)\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Somatic Alterations widget\n",
    "    df['opcSomaticAlt'] = np.where(df[somaticAltCols.values()].sum(axis=1) > 0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Add TRUE/FALSE for presence of Epigenetic Modifications widget\n",
    "    df['opcEpiMod'] = np.where(df[epigeneticModCols.values()].sum(axis=1) > 0, 'TRUE', 'FALSE')\n",
    "\n",
    "    # Add suffix column\n",
    "    df['suffixUrl'] ='/evidence/'+df['targetId']+'/'+df['diseaseId']\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df1 = df[\n",
    "                ['suffixUrl', \n",
    "                'targetId', \n",
    "                'diseaseId', \n",
    "                'targetNameOT', \n",
    "                'diseaseNameOT', \n",
    "                'opcGeneExp_evidence', \n",
    "                'opcSomaticAlt',\n",
    "                'opcEpiMod', \n",
    "                'snvByGene', \n",
    "                'snvByVariant', \n",
    "                'cnvByGene', \n",
    "                'fusionByGene', \n",
    "                'fusion',\n",
    "                'methylByGene',\n",
    "                'methylByIsoform']\n",
    "                ]\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Pediatric Cancer Data Navigation (PCDN) by Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_pcdnGene(sampleSize:int, \n",
    "                        pcdn_df:pd.DataFrame=pcdn_df, \n",
    "                        priority_targets:pd.DataFrame=priority_targets,\n",
    "                        randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for PCDN page, gene search. Should only include \n",
    "    targets contained within the PCDN data (and therefore new pediatric data). \n",
    "\n",
    "    :param sampleSize: int number of random targets to include in test case\n",
    "    :param pcdn_df: pandas DataFrame of preprocessed Pediatric Cancer Data Navigation page\n",
    "    :param priority_targets: pandas DataFrame of targets to always include in test case\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # The code for original function below is commented out due to unexpected behavior in \n",
    "    # the PCDN search where selecting a symbol (e.g. EGFR) will return results for that symbol \n",
    "    # and any other that starts with the symbol followed by a hyphen (e.g. EGFR-AS1). The test\n",
    "    # case script has been updated to match this behavior, but keeping the commented code in \n",
    "    # case the PCDN search behavior is ever changed. \n",
    "\n",
    "    # # Build subset of the PCDN df using priority targets and random sample of other targets\n",
    "    # random.seed(randomSeed)\n",
    "    # df = pcdn_df[\n",
    "    #     (pcdn_df['Gene_symbol'].isin(priority_targets['symbol'])) |\n",
    "    #     (pcdn_df['Gene_symbol'].isin(\n",
    "    #       random.sample(pcdn_df['Gene_symbol'].unique().tolist(), sampleSize)))]\n",
    "\n",
    "    # # Group subset by gene symbol to get resulting evidence count\n",
    "    # df1 = df.groupby('Gene_symbol').size().reset_index()\n",
    "\n",
    "    # # Rename columns\n",
    "    # df1.rename(columns={0:'evidenceResults', 'Gene_symbol':'name'}, inplace=True)\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "\n",
    "    # Create combined list of test targets from priority targets and random sample\n",
    "    random.seed(randomSeed)\n",
    "    targetList = (priority_targets['symbol'].tolist() + \n",
    "                    random.sample(pcdn_df['Gene_symbol'].unique().tolist(), sampleSize)) \n",
    "    \n",
    "    # Define empty dataframe to fill\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over targets to test to get count of diseases for the target AND for\n",
    "    # any targets that start with the target name followed by a hyphen (-)\n",
    "    for gene in targetList:\n",
    "        count = len(\n",
    "            pcdn_df[\n",
    "            (pcdn_df['Gene_symbol'] == gene) | \n",
    "            (pcdn_df['Gene_symbol'].str.startswith(gene + '-'))])\n",
    "\n",
    "        # Add the combined count for each target to a cumulative dataframe\n",
    "        df_row = pd.DataFrame([{'name': gene, 'evidenceResults': count}])\n",
    "        df = pd.concat([df, df_row])\n",
    "\n",
    "    df1 = df.reset_index(drop=True)\n",
    "\n",
    "    # End updated code section. Keep everything below ----\n",
    "\n",
    "    # Add suffix and category columns\n",
    "    df1['suffixUrl'] = '/pediatric-cancer-data-navigation'\n",
    "    df1['category'] = 'target'\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df2 = df1[\n",
    "                ['suffixUrl', \n",
    "                'category', \n",
    "                'name', \n",
    "                'evidenceResults']\n",
    "                ]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Pediatric Cancer Data Navigation (PCDN) by Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_pcdnDisease(sampleSize:int, \n",
    "                        maxResults:int=10000,\n",
    "                        pcdn_df:pd.DataFrame=pcdn_df, \n",
    "                        priority_diseases:pd.DataFrame=priority_diseases,\n",
    "                        randomSeed:int=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Build test case df for PCDN page, disease search. Should only include \n",
    "    diseases contained within the PCDN data (and therefore new pediatric data). \n",
    "\n",
    "    :param sampleSize: int number of random diseases to include in test case\n",
    "    :param maxResults: int maximum count of evidence results. Default is 10,000 to match MTP\n",
    "    :param pcdn_df: pandas DataFrame of preprocessed Pediatric Cancer Data Navigation page\n",
    "    :param priority_diseases: pandas DataFrame of diseases to always include in test case\n",
    "    :param randomSeed: int seed for reproducible random results\n",
    "    \"\"\"\n",
    "\n",
    "    # Build subset of the PCDN df using priority diseases and random sample of other diseases\n",
    "    random.seed(randomSeed)\n",
    "    df = pcdn_df[\n",
    "        (pcdn_df['Disease'].isin(priority_diseases['name'])) |\n",
    "        (pcdn_df['Disease'].isin(random.sample(pcdn_df['Disease'].unique().tolist(), sampleSize)))]\n",
    "\n",
    "    # Group subset by gene symbol to get resulting evidence count\n",
    "    df1 = df.groupby('Disease').size().reset_index().rename(columns={0:'fullResults'})\n",
    "\n",
    "    # Cap number of results at a maximum (default 10,000) to match site functionality\n",
    "    df1['evidenceResults'] = np.where(df1['fullResults'] > maxResults, maxResults, df1['fullResults'])\n",
    "\n",
    "    # Rename columns\n",
    "    df1.rename(columns={'Disease':'name'}, inplace=True)\n",
    "\n",
    "    # Add suffix and category columns\n",
    "    df1['suffixUrl'] = '/pediatric-cancer-data-navigation'\n",
    "    df1['category'] = 'disease'\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df2 = df1[\n",
    "                ['suffixUrl', \n",
    "                'category', \n",
    "                'name', \n",
    "                'evidenceResults']]\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: Pediatric Molecular Targets Lists (PMTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testCase_pmtl(pmtl_df:pd.DataFrame=pmtl_df):\n",
    "    \"\"\" \n",
    "    Build test case df for FDA PMTL page.\n",
    "\n",
    "    :param pmtl_df: pandas DataFrame of the computable FDA PMTL\n",
    "    \"\"\"\n",
    "\n",
    "    # Build df that groups PMTL targets by R/NR designation and get totals\n",
    "    df = pmtl_df.groupby('designation').size().reset_index()\n",
    "\n",
    "    # Add suffix and extra column to specify the group type as designation\n",
    "    df['suffixUrl'] = '/fda-pmtl'\n",
    "    df['category'] = 'designation'\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={0:'count', 'designation':'categoryValue'}, inplace=True)\n",
    "\n",
    "    # Rearrange columns and omit redundant\n",
    "    df1 = df[\n",
    "                ['suffixUrl', \n",
    "                'category', \n",
    "                'categoryValue', \n",
    "                'count']\n",
    "                ]\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test case functions to build dataframes for excel export\n",
    "targetAssc = build_testCase_targetAssc(30)\n",
    "targetProfile = build_testCase_targetProfile(30)\n",
    "diseaseAssc = build_testCase_diseaseAssc(15, all_ped_diseases=True)\n",
    "diseaseProfile = build_testCase_diseaseProfile(15, all_ped_diseases=True)\n",
    "evidence = build_testCase_evidence(35)\n",
    "pcdnGene = build_testCase_pcdnGene(30)\n",
    "pcdnDisease = build_testCase_pcdnDisease(30)\n",
    "pmtl = build_testCase_pmtl()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Test Cases to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build df with test generation metadata\n",
    "sourceData = pd.DataFrame({\n",
    "    'Open Targets Version':OT_VERSION, \n",
    "    'CHoP Somatic Alterations Version':OPENPEDCAN_SOMATIC_ALTERATIONS_VERSION,\n",
    "    'CHoP Gene Expression Version':OPENPEDCAN_GENE_EXPRESSION_VERSION,\n",
    "    'CHoP Epigenetic Modification Version':OPENPEDCAN_EPIGENETIC_MODIFICATION_VERSION,\n",
    "    'CHoP Differential Expression Version':OPENPEDCAN_DIFFERENTIAL_EXPRESSION_VERSION,\n",
    "    'PMTL Version':PMTL_VERSION,\n",
    "    'Test Case Generation Date':pd.Timestamp.now(),\n",
    "    'Reproducible Random Seed':RANDOM_SEED\n",
    "    }.items(),\n",
    "    columns=['MTP DV3 TEST CASES FOR AUTOMATION', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and name test case dfs for export\n",
    "outputDict = {\n",
    "    'sourceData': sourceData,\n",
    "    'targetAssc': targetAssc,\n",
    "    'targetProfile': targetProfile,\n",
    "    'diseaseAssc': diseaseAssc,\n",
    "    'diseaseProfile': diseaseProfile,\n",
    "    'evidence': evidence,\n",
    "    'pcdnGene': pcdnGene,\n",
    "    'pcdnDisease': pcdnDisease,\n",
    "    'pmtl': pmtl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_test_cases_as_excel(outputDict:dict, outfile:str=XLSX_OUTPUT):\n",
    "    \"\"\"\n",
    "    Build and format an Excel file containing test cases for automation.\n",
    "    \n",
    "    :param outfile: filepath for Excel output file\n",
    "    :param outputDict: dict of test cases to export as Excel sheets\n",
    "    \"\"\"\n",
    "\n",
    "    # Create writer using XlsxWriter as the engine\n",
    "    writer = pd.ExcelWriter(outfile, engine='xlsxwriter')\n",
    "\n",
    "    # Write each test case df defined in outputDict to a sheet\n",
    "    for sheetname, df in outputDict.items():\n",
    "        df.to_excel(writer, sheet_name=sheetname, index=False)\n",
    "    \n",
    "    # Format width of (descriptive) first sheet columns for readability\n",
    "    writer.sheets[list(outputDict.keys())[0]].set_column(0, 0, 40)\n",
    "    writer.sheets[list(outputDict.keys())[0]].set_column(1, 1, 20)\n",
    "\n",
    "    # Save and close excel writer\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run export function\n",
    "export_test_cases_as_excel(outputDict, XLSX_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "431f20539060e4195af057caf518edb196c7ace9bef336047b9965dbe49805af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
